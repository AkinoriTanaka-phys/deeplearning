{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "30ff45ff",
      "metadata": {
        "id": "30ff45ff"
      },
      "source": [
        "# Gridworldで強化学習\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "153d6843",
      "metadata": {
        "id": "153d6843"
      },
      "source": [
        "## ダウンロード\n",
        "\n",
        "まずはデモ用のパッケージ\n",
        "- https://github.com/AkinoriTanaka-phys/gridworld_rl\n",
        "\n",
        "をダウンロードします。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5084e6b1",
      "metadata": {
        "id": "5084e6b1"
      },
      "source": [
        "### Google colaboratory から試す場合\n",
        "\n",
        "以下のセルを実行するとセッションストレージにダウンロードします："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b26391d",
      "metadata": {
        "id": "9b26391d"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/AkinoriTanaka-phys/gridworld_rl.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ae92e1a",
      "metadata": {
        "id": "6ae92e1a"
      },
      "source": [
        "左側のフォルダマークをクリックすると、\n",
        "- `gridworld_rl`（こちらが上のセルでダウンロードしたもの）\n",
        "- `sample_data`（こちらはデフォルト）\n",
        "\n",
        "と表示されるはずです。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3317c2a",
      "metadata": {
        "id": "a3317c2a"
      },
      "source": [
        "### 手元のPC から試す場合\n",
        "\n",
        "notebookのpythonカーネルが以下のライブラリを持っている必要があります：\n",
        "- numpy\n",
        "- matplotlib\n",
        "- copy\n",
        "\n",
        "万が一これらが入っていない場合は適宜入れてください。\n",
        "\n",
        "このnotebookの直下にダウンロードしますが、別の場所に置きたい場合は適宜修正してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da4c6734",
      "metadata": {
        "id": "da4c6734"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "git clone https://github.com/AkinoriTanaka-phys/gridworld_rl.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "342122f8",
      "metadata": {
        "id": "342122f8"
      },
      "source": [
        "## 読み込み\n",
        "\n",
        "今回使うクラスをインポートしておきます："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "896f87aa",
      "metadata": {
        "id": "896f87aa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from gridworld_rl.src.env import GridWorld, Maze\n",
        "from gridworld_rl.src.agt import You, Agent, Softmax, EpsilonGreedy, Value\n",
        "from gridworld_rl.src.opt import ActorCritic, REINFORCE, MonteCarlo, SARSA, Qlearning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "それぞれの詳細は`help()`で参照してみてください："
      ],
      "metadata": {
        "id": "za8vqZq-nfnw"
      },
      "id": "za8vqZq-nfnw"
    },
    {
      "cell_type": "code",
      "source": [
        "help(REINFORCE)"
      ],
      "metadata": {
        "id": "OnlraH7SnmpY"
      },
      "id": "OnlraH7SnmpY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a1b42070",
      "metadata": {
        "id": "a1b42070"
      },
      "source": [
        "## 環境\n",
        "\n",
        "環境は `env.py` で定義されています。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d4c4f3e",
      "metadata": {
        "id": "5d4c4f3e"
      },
      "outputs": [],
      "source": [
        "env = GridWorld(5, 5)\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0c13a0a",
      "metadata": {
        "id": "b0c13a0a"
      },
      "source": [
        "### Maze環境\n",
        "\n",
        "`GridWorld`を継承して、もう少し解くのが難しそうな環境も用意しました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "422e6c84",
      "metadata": {
        "id": "422e6c84"
      },
      "outputs": [],
      "source": [
        "env = Maze(5, 5, sparcity=1.3)\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8af6a8ce",
      "metadata": {
        "id": "8af6a8ce"
      },
      "source": [
        "## エージェント\n",
        "\n",
        "エージェントは `agt.py` で定義しています。エージェントには環境 `env` を食わせます。\n",
        "\n",
        "環境の振る舞いを調べるために、ユーザー自身をエージェントとして動かすことができます。エージェントの名前は`You`です："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8c96354",
      "metadata": {
        "id": "c8c96354"
      },
      "outputs": [],
      "source": [
        "agt = You(env)\n",
        "\n",
        "env.reset()\n",
        "while not env.is_terminated():\n",
        "    env.render()\n",
        "    s = env.get_state()\n",
        "    a = agt.play(s)\n",
        "    sn, rn, _, _ = env.step(a)\n",
        "    print(f\"reward={rn}\")\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7c463c2",
      "metadata": {
        "id": "e7c463c2"
      },
      "source": [
        "### 学習エージェント\n",
        "\n",
        "学習用のエージェントは `Agent` に適当な方策を与えて定義します。`agt.py` では\n",
        "- `Softmax`（方策勾配法向き）\n",
        "- `EpsilonGreedy`（それ以外向き、デフォルトε=0.1）\n",
        "\n",
        "を定義しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3db7a9fa",
      "metadata": {
        "id": "3db7a9fa"
      },
      "outputs": [],
      "source": [
        "agt = Agent(env, Policy_class=Softmax)\n",
        "\n",
        "env.reset()\n",
        "while not env.is_terminated():\n",
        "    env.render()\n",
        "    s = env.get_state()\n",
        "    a = agt.play(s)\n",
        "    sn, rn, _, _ = env.step(a)\n",
        "    print(f\"move={env.ACTION2STR[a]}, reward={rn}\")\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad44dd5f",
      "metadata": {
        "id": "ad44dd5f"
      },
      "source": [
        "## 強化学習アルゴリズム\n",
        "\n",
        "強化学習アルゴリズムは `opt.py` で定義しています。 アルゴリズムにはエージェント `agt` を食わせます。\n",
        "\n",
        "現行のモデルは `GridWorld` や `Maze` の座標ごとに4種の行動（up, down, left, right）についてのパラメータを持っているものですが、ニューラルネットなどを用いて一般化することも可能かと思います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa831d27",
      "metadata": {
        "id": "fa831d27"
      },
      "outputs": [],
      "source": [
        "# ちょっと大きな迷路を作っておきます\n",
        "env = Maze(15, 15, sparcity=1.3)\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76d598af",
      "metadata": {
        "id": "76d598af"
      },
      "source": [
        "## 方策勾配法\n",
        "\n",
        "方策勾配法は\n",
        "- `REINFORCE`\n",
        "    - オプションとしてエージェント定義で `Value_class=Value` を指定するとベースライン込みのREINFORCEとなり、学習が安定化します。\n",
        "- `ActorCritic`\n",
        "    - こちらはエージェント定義で `Value_class=Value` の指定が必須です。\n",
        "\n",
        "を用意しました。両方ともエージェント定義で `Policy_class=Softmax` の使用が想定されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c343a048",
      "metadata": {
        "scrolled": true,
        "id": "c343a048"
      },
      "outputs": [],
      "source": [
        "N_episode = 1000\n",
        "\n",
        "agt = Agent(env, Policy_class=Softmax, Value_class=Value)\n",
        "#opt = REINFORCE(agt, eta_p=.5, eta_v=.5, gamma=.99)\n",
        "opt = ActorCritic(agt, eta_p=.5, eta_v=.5, gamma=.99)\n",
        "\n",
        "for episode in range(N_episode):\n",
        "    env.reset()\n",
        "    opt.reset()\n",
        "    while not env.is_terminated():\n",
        "        s = env.get_state()\n",
        "        a = agt.play(s)\n",
        "        sn, rn, _, _ = env.step(a)\n",
        "        opt.step(s, a, rn, sn)\n",
        "    opt.flush() "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f1410fa",
      "metadata": {
        "id": "3f1410fa"
      },
      "source": [
        "訓練後に解かせてみる："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1bd6940",
      "metadata": {
        "id": "e1bd6940"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "env.play(agt)\n",
        "env.render(values_table=agt.policy.param.table)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "422f0432",
      "metadata": {
        "id": "422f0432"
      },
      "source": [
        "## 価値反復法\n",
        "\n",
        "価値＝$\\langle r_1 + \\gamma \\cdot r_2 + \\gamma^2 \\cdot r_2 + \\cdots \\rangle$ の推定に基づく手法は\n",
        "- `MonteCarlo`\n",
        "    - デフォルトは素朴に1エピソード毎にモンテカルロ計算。理論からやや外れますが、アルゴリズム定義のオプションで `erase_history=False` とすると、訪問回数のサンプルを累積する設定になり、性能が上がります。\n",
        "- `SARSA`\n",
        "- `Qlearning`\n",
        "\n",
        "を用意しました。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a41ec05",
      "metadata": {
        "scrolled": false,
        "id": "2a41ec05"
      },
      "outputs": [],
      "source": [
        "N_episode = 1000\n",
        "\n",
        "agt = Agent(env, Policy_class=EpsilonGreedy)\n",
        "#opt = MonteCarlo(agt, gamma=.99, erase_history=False)\n",
        "#opt = SARSA(agt, eta=.5, gamma=.99)\n",
        "opt = Qlearning(agt, eta=.5, gamma=.99)\n",
        "\n",
        "for episode in range(N_episode):\n",
        "    env.reset()\n",
        "    opt.reset()\n",
        "    while not env.is_terminated():\n",
        "        s = env.get_state()\n",
        "        a = agt.play(s)\n",
        "        sn, rn, _, _ = env.step(a)\n",
        "        opt.step(s, a, rn, sn)\n",
        "    opt.flush() \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76822f24",
      "metadata": {
        "id": "76822f24"
      },
      "source": [
        "訓練後に解かせてみる："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bf36090",
      "metadata": {
        "id": "9bf36090"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "env.play(agt)\n",
        "env.render(values_table=agt.policy.param.table)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "colab": {
      "name": "rl_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
