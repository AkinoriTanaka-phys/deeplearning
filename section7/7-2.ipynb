{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def random_X(seq_length=4, vec_dim=4):\n",
    "    return [[0]+np.random.choice([0, 1], vec_dim).tolist() for i in range(seq_length)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-2. tensorflowでRNNをカスタマイズする方法\n",
    "\n",
    "その前に、自分独自のリカレントニューラルネットワークをtensorflowで作る方法を紹介します。tensorflow実装での説明ですから、他のライブラリだと異なる方法を採るかも知れませんが、基本となる考え方は同じです。\n",
    "\n",
    "既に少し触れたように、tensorflowではRNNを\n",
    "* `cell = tf.keras.layers.Layer()オブジェクト`として\n",
    "* `RNN_Layer = tf.keras.layers.RNN(cell)`とする\n",
    "\n",
    "の2ステップを踏んで作るように設定されているようです。2つめの`tf.keras.layers.RNN(cell)`は勝手にこれまでのようなRNNを作るための処理をしてくれるということで、自分でカスタマイズする場合の主な目標は**cellオブジェクトを作る**ことだと言えます。\n",
    "\n",
    "### Cell で何を実装すればよいか\n",
    "Cellでは各時刻で使う訓練パラメータと各時刻での処理を定義します。\n",
    "\n",
    "![alt](2.jpg)\n",
    "\n",
    "図に描いたように、`call()`という関数で時刻 t での処理を定義します。内部状態はリストで取り扱うようです。それと`@property`でデコレートした、内部状態の次元のリストを返す関数`state_size()`も定義しておく必要があるようです$^※$。`bulid()`はいらないかもしれません。\n",
    "\n",
    "> $^※$ `state_size()`について、ベクトルじゃなくてテンソル量を内部状態にしたい場合は、スカラー値ではなく`tf.TensorShape([dim_A, dim_B, ...])`をリストの要素として書けば良いようです。この処理を後で使います。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装してみる\n",
    "リカレントニューラルネットワークとして最も単純な\n",
    "\n",
    "![alt](3.jpg)\n",
    "\n",
    "のようなRNNレイヤー(MySimpleRNN)のためのCellを作るとズバリ以下のようになります："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MySimpleRNNCell(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(MySimpleRNNCell, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.W= tf.keras.layers.Dense(self.units)\n",
    "        super(MySimpleRNNCell, self).build(input_shape[-1])\n",
    "        self.build = True\n",
    "        \n",
    "    def call(self, inputs, states):\n",
    "        ''' inputs:xt, states:[h{t-1}], each vector shape = [batchsize, vec_dim]'''\n",
    "        x, h_prev = inputs, states[0]\n",
    "        xh = tf.concat([x, h_prev], axis=1)\n",
    "        h = tf.keras.activations.tanh(self.W(xh))\n",
    "        return h, [h] # output, next_states\n",
    "        \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        ''' list of dimensions of states '''\n",
    "        return [self.units]\n",
    "    \n",
    "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "        ''' how to initialize states '''\n",
    "        h0 = tf.zeros([batch_size, self.units], dtype)\n",
    "        return [h0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cellは基本的には\n",
    "* `tf.keras.layers.Layer`を継承したクラスとして実装\n",
    "* 内部状態をpythonのリストとして表現する\n",
    "* `call(self, inputs, states)` で各時系列の時刻$t$における処理を書く。このときの`states`は前の時刻が終わった直後の内部状態リスト\n",
    "* `@property`でデコレートした関数`state_size(self)`で内部状態の次元のリストを返すようにする\n",
    "* `get_initial_state`で内部状態の初期化の仕方を指定する\n",
    "\n",
    "を満たせばどのような実装でもOKです。\n",
    "\n",
    "### 実際に使う\n",
    "実際に使う際は、`cell`を`RNN_layer = tf.keras.layers.RNN(cell)` のようにしてラップします。以下のように\n",
    "\n",
    "![alt](32.jpg)\n",
    "\n",
    "cellを時系列データ処理の時間方向につないだ縦向きの一つのニューラルネットワーク層を作るのが`tf.keras.layers.RNN()`です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力\n",
      " tf.Tensor(\n",
      "[[[0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 0.]]], shape=(1, 2, 11), dtype=float32)\n",
      "出力\n",
      " [<tf.Tensor: shape=(1, 2, 3), dtype=float32, numpy=\n",
      "array([[[-0.7203732 , -0.7506157 ,  0.60109067],\n",
      "        [-0.6228929 ,  0.71592975, -0.40907428]]], dtype=float32)>, <tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-0.6228929 ,  0.71592975, -0.40907428]], dtype=float32)>]\n",
      "状態\n",
      " <tf.Variable 'rnn_1/Variable:0' shape=(1, 3) dtype=float32, numpy=array([[-0.6228929 ,  0.71592975, -0.40907428]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "cell      = MySimpleRNNCell(units=3)\n",
    "RNN_layer =  tf.keras.layers.RNN(cell, return_sequences=True, return_state=True, stateful=True) # いろいろオプションあり\n",
    "\n",
    "X = tf.constant([random_X(seq_length=2, vec_dim=10)], dtype=tf.float32)\n",
    "print(\"入力\\n\", X)\n",
    "print(\"出力\\n\", RNN_layer(X))\n",
    "print(\"状態\\n\", RNN_layer.states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.keras.layers.RNN`でレイヤーを作る際に、いくつかオプションがあります：\n",
    "* `return_sequences=True/False`: 入力 X の時系列の時刻ごとの出力をするかしないか\n",
    "* `return_state=True/False`: 内部状態を出力するかしないか(`return_sequences=True`でも、最終時刻の内部状態しか出ないっぽいです)\n",
    "* `stateful=True/False`: 内部状態を保持するかしないか。保持しない場合は処理毎に初期化(なぜかNone?)される。\n",
    "\n",
    "より詳しくはtensorflowのドキュメントを読んでいただければと思います。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
