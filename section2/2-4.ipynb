{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# no preamble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-4. 赤池情報量基準と汎化\n",
    "### 教師なし学習における汎化\n",
    "さて、ここまでで、\n",
    "\n",
    "本当に最小化したいもの(<span style=\"color: red; \">汎化誤差</span>とよばれる)|仕方無く最小化するもの(<span style=\"color: blue; \">経験誤差</span>とよばれる)\n",
    ":---:|:---:\n",
    "$\\color{red}{D_{KL}(p\\mid\\mid q_{\\Theta_{ML}^{(N)}})} \\approx \\frac{dim(\\Theta)}{2N}$|$\\color{blue}{- \\frac{\\log P_N(\\Theta_{ML}^{(N)})}{N}}-S(p) \\approx - \\frac{dim(\\Theta)}{2N}$\n",
    "\n",
    "であることが示唆されてきましたが、エントロピー $S(p)$ は常に正であることを使うと、\n",
    "\n",
    "$$\n",
    "{\\color{red}{D_{KL}(p\\| q_{\\Theta_{ML}^{(N)}})}}\\ \\lesssim {\\color{blue}{- \\frac{\\log P_N(\\Theta_{ML}^{(N)})}{N}}} + \\frac{dim(\\Theta)}{N}\n",
    "$$\n",
    "\n",
    "となっていることがわかります。実際、この不等式は $\\Theta=$(モデル確率分布に含まれる全パラメータ)として\n",
    "- モデル $q_\\Theta$ の[Fisher情報量行列](https://ja.wikipedia.org/wiki/フィッシャー情報量)が非退化\n",
    "- モデル $q_\\Theta$ が真の分布 $p$ を含む\n",
    "- 全ての最尤推定量が**不偏推定量**になっている\n",
    "\n",
    "場合に**ガウス分布に限らず、どんな場合でも成り立ちます**([付録hoge](#)参照)。この式をもちいて機械学習の**汎化**について説明すると以下のようになります\n",
    "\n",
    "1. <span style=\"color: red; \">汎化誤差</span>$\\Big({\\color{red}{D_{KL}(p\\| q_{\\Theta})}}\\Big)$をなるべくゼロに近づけたいが、計算できない。\n",
    "2. 代わりに<span style=\"color: blue; \">経験誤差</span>$\\Big( {\\color{blue}{- \\frac{\\log P_N(\\Theta)}{N}}}\\Big)$ をなるべく小さくする $\\Theta$ を探す。\n",
    "3. 2を達成するためには $dim(\\Theta)$ を大きくして、モデルのフィッティング能力を上げたい。\n",
    "4. しかし $dim(\\Theta)$ が大きすぎると、いくら<span style=\"color: blue; \">経験誤差</span>$\\Big({\\color{blue}{- \\frac{\\log P_N(\\Theta)}{N}}}\\Big)$ を小さくしても、上の不等式の二項目 $\\Big(\\frac{dim(\\Theta)}{N}\\Big)$ が大きくなってしまい、<span style=\"color: red; \">汎化誤差</span>$\\Big({\\color{red}{D_{KL}(p\\| q_{\\Theta})}}\\Big)$ を小さくしたことにならない。\n",
    "\n",
    "そういう意味では、右辺をなるべく小さくするようなモデル $q_\\Theta$ を選ぶのが良さそうだとわかります。実際この「モデル選択の基準」はよく使われており、[赤池情報量基準(Akaike's Information Criterion)](https://ja.wikipedia.org/wiki/赤池情報量規準) と呼ばれています。\n",
    "\n",
    "> 実際には右辺 $\\times N$ の量：\n",
    "$$\n",
    "{\\color{blue}{- \\log P_N(\\Theta_{ML}^{(N)})}} + dim(\\Theta)=:AIC(q_{\\Theta_{ML}^{(N)}})\n",
    "$$\n",
    "をAICと定義するようです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 教師あり学習における汎化\n",
    "ここまでの話は、$p({\\bf x})$ でデータが生成されている仮定＝**教師なし学習** における汎化の話だったと考えることができます。教師あり学習もほとんど同様に議論できます。まずデータを生成する確率 $p({\\bf x}, y)$ があると考え、データはそこからの独立なサンプルだと考えるのがいいでしょう：\n",
    "\n",
    "$$\n",
    "({\\bf x}_1,  y_1), ({\\bf x}_2,  y_2), \\dots, ({\\bf x}_N,  y_N) \\sim p({\\bf x}, y)\n",
    "$$\n",
    "\n",
    "このとき、経験確率\n",
    "\n",
    "$$\n",
    "\\hat{p}_N(x,y)= \\frac{1}{N} \\sum_{n=1}^N \\delta ({\\bf x} - {\\bf x}_n) \\delta(y - y_n)\n",
    "$$\n",
    "\n",
    "は、教師なし学習のときと同様にサンプル数 $N\\to\\infty$ でもとの $p({\\bf x}, y)$ に収束するはずです。これまで同様、有限の $N$ だけから、もとの $p({\\bf x}, y)$ を予想できれば学習完了というわけです。\n",
    "\n",
    "ところで教師あり学習の場合、${\\bf x}$ が与えられた時の答え $y$ は何ですか、というのが問題意識なため、どんな ${\\bf x}$ が出てきやすいかは余り興味がありません。そこでモデルは\n",
    "\n",
    "$$\n",
    "q_\\theta(y|{\\bf x})\n",
    "$$\n",
    "\n",
    "の条件付き確率を考えることになります。\n",
    "強いて同時確率にしたければ、${\\bf x}$ は天から与えられるものとし\n",
    "\n",
    "$$\n",
    "q_\\theta({\\bf x}, y) = q_\\theta(y|{\\bf x}) p({\\bf x})\n",
    "$$\n",
    "\n",
    "として問題ないでしょう。従って目標は\n",
    "$$\n",
    "\\left. \\begin{array}{ll}\n",
    "D_{KL}(p\\|q_\\theta)&=\n",
    "\\int p({\\bf x}, y)\n",
    "\\log \\frac{p({\\bf x}, y)}{q_\\theta({\\bf x}, y)} d{\\bf x} dy\n",
    "\\\\\n",
    "&= \n",
    "\\int p({\\bf x},y)\n",
    "\\log \\frac{p({\\bf x},y)}{q_\\theta(y|{\\bf x}) p({\\bf x})} dx dy\n",
    "\\\\\n",
    "&=\n",
    "\\int p({\\bf x},y)\n",
    "\\log \\frac{p(y|{\\bf x})}{q_\\theta(y|{\\bf x})} d{\\bf x} dy\n",
    "\\\\\n",
    "&= -S_{y|{\\bf x}}(p) -\\langle \\log q_\\theta(y|{\\bf x}) \\rangle_{p({\\bf x},y)}\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "の値をなるべく小さくする $\\theta$ を見つけることです\n",
    "> ここで$S_{y|{\\bf x}}(p)$は条件付きエントロピー\n",
    "$$\n",
    "S_{y|{\\bf x}}(p):=-\\Big\\langle \\log \\frac{p({\\bf x},y)}{p({\\bf x})} \\Big\\rangle_{p({\\bf x},y)}=- \\langle \\log p(y|{\\bf x}) \\rangle_{p({\\bf x},y)}\n",
    "$$\n",
    "です。\n",
    "\n",
    "教師なしのときと同様に、この値は計算不能なので代わりに最尤推定をもちい、経験誤差\n",
    "\n",
    "$$\n",
    "-\\langle \\log q_\\theta(y|{\\bf x}) \\rangle_{\\hat{p}_N({\\bf x}, y)}\n",
    "$$\n",
    "\n",
    "の値を減らしにかかります。この場合も $({\\bf x},y)$ の変数をひとまとまりにして $X$ とでも置けば教師なし学習のとき同様に以下が成立することがわかります：\n",
    "\n",
    "$$\n",
    "\\left. \\begin{array}{ll}\n",
    "{\\color{red} {D_{KL}(p\\|q_{\\theta_{ML}^{(N)}})}}&\\lesssim\n",
    "{\\color{blue}{-\\langle \\log q_{\\theta_{ML}^{(N)}} (y|{\\bf x}) \\rangle_{\\hat{p}_N}}}\n",
    "+\n",
    "\\frac{dim(\\theta)}{N}\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "が成り立ち、こちらでも経験誤差最小化とモデルの複雑さの間のトレードオフがあることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深層学習における汎化の不思議\n",
    "まだ深層学習の話を説明していませんが、後に説明するように深層ニューラルネットは$q_\\Theta$として表現することができます。またその学習も最尤推定(のようなもの$^*$)と考えることができます。\n",
    ">$\\ ^*$おいおい説明しますが、深層学習の学習アルゴリズムはこの節で取り扱ってきたような厳密な最尤推定とは異なり、ある意味で「手抜きの」最尤推定になっています。\n",
    "\n",
    "厳密な意味での最尤推定でないモデルにそのままAICを当てはめるのはやってはいけないのですが、試しに無理やり当てはめてみると何が起こるか説明して終わりにしたいと思います。\n",
    "\n",
    "うまく学習させ、汎化した(つまり{\\color{red}汎化誤差}が小さいと思われる)モデルのパラメータ数 $dim(\\Theta)$ が、深層学習では巨大すぎるということです。\n",
    "例えばILSVRC'15優勝モデルのResNet([arXiv:1512.03385](https://arxiv.org/abs/1512.03385))をCIFAR-10($N=50,000$)と呼ばれるデータで学習させた場合で最も多いパラメータ数のモデルは\n",
    "\n",
    "$$\n",
    "dim(\\Theta) =19.4M = 19,400,000\n",
    "$$\n",
    "\n",
    "です。これをAICに当てはめると\n",
    "\n",
    "$$\n",
    "\\color{red}{D_{KL}(p\\| q_{\\Theta_{DL}^{(N)}})}\\ \\lesssim \\underbrace{\\color{blue}{- \\frac{\\log P_N(\\Theta_{DL}^{(N)})}{N}}}_{\\text{very small}} + \\underbrace{\\frac{dim(\\Theta)=19,400,000}{N=50,000}}_{388}\n",
    "$$\n",
    "\n",
    "となり、<span style=\"color: red; \">汎化誤差</span>$\\left(\\color{red}{D_{KL}(p\\| q_{\\Theta})}\\right)$の小ささを保証できないため、**ほとんど意味のない過剰適合したモデルになると期待されます**。しかし論文によると知らないデータ(test data)に対する誤認識率は $7.93\\%$ と非常に小さく、期待に大きく反します。\n",
    "このことは未だうまい解釈のない、深層学習における謎のうちの一つなのですが、以下ではこれに関するいくつかのコメントをしてみます。\n",
    "\n",
    "#### コメント1:不等式の評価が甘い可能性\n",
    "付録\\ref{sec:aic}での導出ではサンプル数 $N$ について主要に聞いてくる項だけを取り出し、後の項は無視しています。これが悪さをしているのかもしれません。例えば $x$ が小さい領域で\n",
    "\n",
    "$$\n",
    "1 - x \\approx e^{-x}\n",
    "$$\n",
    "\n",
    "ですが、これをそのまま $x$ が大きな領域で使おうとすると、主要項だけで近似した左辺は大きな値になりますが、すべての項を考慮した右辺は小さくなります。このようなことがないとも限りませんし、実際に $N$ の有限性からくる補正の[研究論文(直リンク)](https://www.tandfonline.com/doi/abs/10.1080/03610927808827599)などもあります\n",
    ">更に興味深い(けれど眉唾な)主張：モデルの複雑さを高めてゆくと「過剰適合の向こう側」に汎化誤差が減り続ける新領域があるという報告([arXiv:1812.11118](https://arxiv.org/abs/1812.11118))もあります。線形回帰に限定して同現象を考察した論文([arXiv:1903.08560](https://arxiv.org/abs/1903.08560)もあります。これらの論文は非常に興味深いですが、深層学習に見られる汎化をこれで説明できたことになるのかは、まだ研究の余地があることなのではないかと思います。また、深層学習に関してこの手の研究成果もいくつかあります。\n",
    "\n",
    "#### コメント2:深層学習は厳密な最尤推定ではない\n",
    "ここまで最尤推定を前提に説明してきました。\n",
    "深層学習の学習でも、負の対数尤度を減らすようにパラメータを調整すると解釈できますが、真の最小値を求めることはしませんし、計算量の問題により求められません。加えて、様々な正則化や正規化を用いるため純粋な最尤推定が行われるケースはむしろ稀、いや、皆無であると言える気がします。\n",
    "\n",
    "#### コメント3:深層学習ではパラメータが多そうに見えて実は少ない？\n",
    "深層ニューラルネットには膨大なバラメータがありますが、もとのパラメータ$\\Theta = \\{ \\theta^i\\}$のほとんどは意味のないパラメータかもしれません。実際にモデルに寄与するパラメータ$\\Phi = \\{ \\varphi^j \\}$とし、数を比べると\n",
    "\n",
    "$$\n",
    "dim(\\Theta) \\gg dim(\\Phi)\n",
    "$$\n",
    "\n",
    "となれば、ここまでの話と辻褄が合うかもしれません。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
