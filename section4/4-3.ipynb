{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import SimpleRNN, GRU, LSTM\n",
    "\n",
    "def get_elocutions(path):\n",
    "    ''' 全ての台詞を取り出す処理 '''\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        ls = f.readlines()\n",
    "        #print(ls)\n",
    "        start_elocution = True\n",
    "        elocutions = []\n",
    "        for i in range(len(ls)):\n",
    "            if start_elocution:\n",
    "                elocution = [ls[i]]\n",
    "                start_elocution = False\n",
    "            elif ls[i]=='\\n':\n",
    "                elocution.append('*')\n",
    "                elocutions.append(\"\".join(elocution))\n",
    "                start_elocution = True\n",
    "            else:\n",
    "                elocution.append(ls[i])\n",
    "                start_elocution = False\n",
    "    return elocutions\n",
    "\n",
    "def return_processed(X, T, lower_bound=10):\n",
    "    ''' 入力Xに対し、lower_bound < 文字の長さ < T の台詞のみ取る処理 '''\n",
    "    if len(X)>T or len(X)<lower_bound:\n",
    "        return None\n",
    "    else:\n",
    "        return X.ljust(T, '*')\n",
    "\n",
    "def preprocessing(corpus, T=80):\n",
    "    ''' corpusの台詞X毎に return_processed をかける '''\n",
    "    preprocessed_corpus = []\n",
    "    for X in corpus:\n",
    "        X_ = return_processed(X, T)\n",
    "        if X_ is not None:\n",
    "            preprocessed_corpus.append(X_)\n",
    "    return preprocessed_corpus\n",
    "\n",
    "def get_corpus(path, T=80):\n",
    "    ''' 前処理したコーパスを返す '''\n",
    "    corpus = get_elocutions(path)\n",
    "    return preprocessing(corpus, T=T)\n",
    "\n",
    "class Generator(tf.keras.Model): # モデル設計\n",
    "    def sample_from(self, start_string, num_string=100):\n",
    "        ''' This function is derived from the function `generate_text` in\n",
    "        https://www.tensorflow.org/tutorials/text/text_generation \n",
    "        which is licenced under Apache 2.0 License. '''\n",
    "        input_nums = tf.expand_dims([char2idx[c] for c in start_string], 0)\n",
    "        text_generated = []\n",
    "        model.reset_states()\n",
    "        for i in range(num_string):\n",
    "            if char2idx['*'] in input_nums.numpy():\n",
    "                break\n",
    "            #print(input_nums)\n",
    "            z = self(input_nums)[:,-1,:]\n",
    "            predicted_id = tf.random.categorical(z, num_samples=1) # このサンプリングは logit を指定して softmax からサンプリング\n",
    "            text_generated.append(tf.squeeze(predicted_id).numpy())\n",
    "            input_nums = predicted_id\n",
    "        print(start_string+\"\".join([idx2char[g] for g in text_generated]))\n",
    "        \n",
    "def get_data_dicts():\n",
    "    ''' This function is derived from functions prepareing `dataset` object in\n",
    "        https://www.tensorflow.org/tutorials/text/text_generation \n",
    "        which is licenced under Apache 2.0 License. '''\n",
    "    path = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "    corpus = get_corpus(path); chars = sorted(set(str(\"\".join(corpus))))\n",
    "    char2idx = {u:i for i, u in enumerate(chars)}; idx2char = np.array(chars)\n",
    "    corpus_num = np.array([[char2idx[c] for c in n] for n in corpus])\n",
    "    D = tf.data.Dataset.from_tensor_slices(corpus_num)\n",
    "    f = lambda n: (n[:-1], n[1:])\n",
    "    D = D.map(f)\n",
    "    return D, chars, char2idx, idx2char\n",
    "    \n",
    "def loss_sum(y, z):\n",
    "    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y, z, from_logits=True)) # z=model(x) は logit\n",
    "\n",
    "def update(X, Y, model, optimizer): # 学習ステップ\n",
    "    with tf.GradientTape() as tape:\n",
    "        Z = model(X) \n",
    "        loss_value = loss_sum(Y, Z) \n",
    "    grads = tape.gradient(loss_value, model.trainable_variables) \n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-3. 時系列データと再帰的な構造\n",
    "まずは前節のシェイクスピアデータを読み込みます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D, chars, char2idx, idx2char = get_data_dicts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "過去の状態を考慮に入れるニューラルネットワークとして、\n",
    "\n",
    "$$\n",
    "q_\\theta(n_{t+1} \\mid [n_0, n_1, \\dots, n_t])\n",
    "$$\n",
    "\n",
    "の形式のモデルはグラフで書くと、\n",
    "\n",
    "![alt](rnn.jpg)\n",
    "\n",
    "のように、時間方向にも矢印が伸びるようなネットワークで表せます。このような **時間方向に自己相互作用する** ニューラルネットワークを **リカレントニューラルネットワーク(RNN)** と呼びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 素朴なRNN\n",
    "まずは#で作った `class Markov` を拡張したモデル `MyRNN` を考えてみましょう：\n",
    "\n",
    "![alt](rnn2.jpg)\n",
    "\n",
    "以前までは2層目は `Dense` でしたが、そこを `SimpleRNN` というのに変更しました。この層は毎時刻 $t$ の出力ベクトル ${\\bf h}_t$ が次の時刻に（内部で）引き継がれるようになっています。式で書くと以下のようになります：\n",
    "\n",
    "$$\n",
    "\\left\\{ \\begin{array}{ll}\n",
    "{\\bf e}_t={\\color{red}{l_{emb}}}(n_t) & \\color{red}{\\text{Embedding}}\n",
    "\\\\\n",
    "{\\bf h}_t = {\\color{red}{f_{rnn}}}({\\bf e}_t)=\\tanh({\\color{red}{W_{he}}}{\\bf e}_t + {\\color{red}{W_{hh}}} {\\bf h}_{t-1} + {\\color{red}{b_h}}) & \\color{red}{\\text{SimpleRNN}}\n",
    "\\\\\n",
    "{\\bf z}_t = {\\color{red}{l_z}}({\\bf h}_t) = {\\color{red}{W_z}} {\\bf h}_t + {\\color{red}{b_z}} & \\color{red}{\\text{Dense}}\n",
    "\\\\\n",
    "q_{\\color{red}{\\theta}}(n\\mid [n_0, n_1, \\dots, n_t]) = [{\\bf \\color{blue}{\\sigma}}({\\bf z}_t)]_{n\\text{-th component}} & \\color{blue}{\\text{Softmax}}\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n",
    "このような時系列の処理を、tensorflowでは\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyRNN(Generator): # モデル設計\n",
    "    ''' This model definition is derived from the model defined in\n",
    "        https://www.tensorflow.org/tutorials/text/text_generation \n",
    "        which is licenced under Apache 2.0 License. '''\n",
    "    def __init__(self, emb_dim, hidden_dim, batch_size, \n",
    "                 RNN_layer=SimpleRNN):\n",
    "        super(MyRNN, self).__init__()\n",
    "        self.l_emb= tf.keras.layers.Embedding(len(chars), emb_dim)\n",
    "        self.f_rnn = RNN_layer(hidden_dim,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True, # ここを True にすると、状態が保持される\n",
    "                        recurrent_initializer='glorot_uniform')\n",
    "        self.l_z = tf.keras.layers.Dense(units=len(chars))\n",
    "        self.build(tf.TensorShape([batch_size, None])) # 上で stateful=True の場合必要\n",
    "    def call(self, nt):\n",
    "        e = self.l_emb(nt)\n",
    "        h = self.f_rnn(e)\n",
    "        z = self.l_z(h)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "のように、内部状態 ${\\bf h}_t$ の引き継ぎ処理を明示的に書くことなく、あたかも ${n \\to {\\bf e} \\to {\\bf h} \\to {\\bf z}}$ の順伝搬かのようにして書くことで実装できます。\n",
    "> このようなリカレント層を自分でカスタマイズしたい場合は親玉？の [`tf.keras.layers.RNN`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN) を継承したクラスを書くのが良いようですが、今回は深入りしないことにします。\n",
    "\n",
    "tensorflowでのRNNは、`tf.keras.layers.RNN`のクラスで実装する限り、時系列データの長さ $T$ を揃えておいて、`numpy`のshapeの意味で\n",
    "- (batchisize, $T$, dim$_{emb}$)\n",
    "\n",
    "のようなデータ形式であれば勝手にデータのテンソルの二成分目を時系列だと認識して処理してくれるようになっています。\n",
    "\n",
    "#### 実装(バッチサイズ)に関する注意\n",
    "`__init__()` の最後の行の処理：`self.build(tf.TensorShape([batch_size, None]))`について注意です。**この実装はモデルを実際にオブジェクトとして作る段階でバッチサイズを指定する仕様になっています** 。これは`RNN_layer(stateful=True)` としていることから外せないようです。ここでstatefulとは ${\\bf h}_t$ のような時間方向のベクトルをミニバッチでSGDしてゆく際に初期化せず、そのまま使い続けるというオプションです。おそらく、バッチサイズ分の状態ベクトル ${\\bf h}_t$ を保持するためのメモリを確保する処理が必要なため`self.build(tf.TensorShape([batch_size, None]))`の宣言が必要なのだと思われます。このいかにも面倒なオプション\n",
    "`RNN_layer(stateful=True)`をここで採用する理由は二つです：\n",
    "- 毎回初期化(`stateful=False`)するとやや処理速度が落ちる\n",
    "- `stateful=True` のほうが結果が良い\n",
    "\n",
    "１つ目の理由は自明ですが、２つ目は何故なのか、よくわかりません。どなたか分かったら教えて下さい。ともかく、ここでの実装ではモデルを作る際に指定したバッチサイズを用いて **のみ** 文章生成が行える様になっているため、バッチサイズを例えば `32` に指定すると、学習後も 32文 を生成することしかできません。これでもいいのですが、ここでは[tensorflowチュートリアル](https://www.tensorflow.org/tutorials/text/text_generation?hl=ja)のやり方を参考に、\n",
    "1. 一旦モデルのパラメータ $\\theta$ を適当なファイルに保存する：`model.save_weights('filename')`\n",
    "2. モデルを使う際は使いたいバッチサイズで新たなモデルを作って、1で保存したパラメータを読み込む：`model.load_weights('filename')`\n",
    "\n",
    "という方式を採ることにします。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimpleRNNで訓練する\n",
    "上の注意に気をつけながら訓練させてみます。またここでは今まで訓練ステップ関数 `update` を定義する際につけていた `@tf.function` のデコレータをやめて、毎回 `tf_update = tf.function(update)` を宣言する仕様にしています。これは何度も同じ環境でグラフ化した関数を使う際にはグラフを作り直さないといけないらしいための処置です。デコレータで関数定義を毎回やり直すよりこちらのほうが一行で書けて良いでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_RNN(RNN_layer = SimpleRNN, epoch_size=10):\n",
    "    tf_update = tf.function(update) # グラフ化（毎回やり直さないとエラーが出る）\n",
    "    batch_size = 32\n",
    "    model = MyRNN(emb_dim=256, hidden_dim=1024, batch_size=batch_size, \n",
    "                  RNN_layer=RNN_layer) # \n",
    "    optimizer=tf.keras.optimizers.Adam()\n",
    "    loss_averages = []\n",
    "    ### training\n",
    "    for epoch in range(epoch_size):\n",
    "        model.reset_states()\n",
    "        batch = D.shuffle(5000).batch(batch_size, drop_remainder=True)\n",
    "        loss_values = []\n",
    "        for (X,Y) in batch:\n",
    "            loss_value = tf_update(X, Y, model, optimizer)\n",
    "            loss_values.append(loss_value)\n",
    "        loss_averages.append(np.average(loss_values))\n",
    "    model.save_weights('RNN_test') # stateful で 設計時にバッチサイズを固定した場合、後で使うなら重みを保存する\n",
    "    return loss_averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際の訓練は以下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 46s, sys: 9.58 s, total: 1min 56s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss_averages = train_RNN(RNN_layer = SimpleRNN, epoch_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimpleRNNでシェイクスピア台詞生成\n",
    "訓練後にモデルを読み込む関数も作っておきます："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_RNN(RNN_layer = SimpleRNN):\n",
    "    ''' This function is derived from functions for loading saved models defined in\n",
    "        https://www.tensorflow.org/tutorials/text/text_generation \n",
    "        which is licenced under Apache 2.0 License. '''\n",
    "    model = MyRNN(emb_dim=256, hidden_dim=1024, batch_size=1, RNN_layer=RNN_layer)\n",
    "    model.load_weights('RNN_test')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "訓練したモデルで台詞生成："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANCANIO:\n",
      "Ay, go dotien, we is so.\n",
      "*\n",
      "AUTHAND: Y ungwer, let's hous.\n",
      "*\n",
      "ANTANIE:\n",
      "That an he shall so thee, epay hi's iffore, as for you?\n",
      "Musid; the wather our any and sheet,\n",
      "ANTHAMUS:\n",
      "Why, how it stand the shall to hour Rime.\n",
      "*\n",
      "A TIUK:\n",
      "What, as he seaseings.\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "model=load_RNN(RNN_layer = SimpleRNN)\n",
    "for _ in range(5):\n",
    "    model.sample_from(\"A\", num_string=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "マルコフなモデルに比べると良いです（例えば、「役名：改行、台詞」のようになっています）が、まだ「英単語っぽくないなにか」が含まれていたりして不満足な気がします。\n",
    "\n",
    "実は素朴な RNN には色々な問題があることが知られています：\n",
    "- **勾配爆発**や**勾配消失**\n",
    "- それに関連し、長期記憶の消失\n",
    "\n",
    "以下では、これらの問題を部分的に解決するとされている2つの定番RNNを紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 長・短期記憶(Long Short-Term Memory, LSTM)\n",
    "\n",
    "まず初めに紹介するのは **長・短期記憶(Long Short-Term Memory, LSTM)** と呼ばれるユニットです。元論文は[こちら](https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735)。LSTMは ${\\bf h}_t$ に加えて、 **内部メモリー状態** ${\\bf c}_t$ を導入し、このメモリ操作：「メモリ忘却、メモリ入力、出力更新」を組み込んだユニットです。このメモリ構造はチューリング機械の構造に似ています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### ゲートの追加\n",
    "メモリの追加に加えて重要なのは **ゲート** と呼ばれるベクトルを用いた特徴量処理のメカニズムです。ゲートベクトル ${\\bf g}$ とし、入力ベクトル ${\\bf x}$ としたとき、ゲートによる処理は **成分毎の積(アダマール積)** $\\odot$ をもちいて\n",
    "\n",
    "$$\n",
    "{\\bf x} \\odot {\\bf g} = [x_1 g_1, x_2 g_2, \\dots]\n",
    "$$\n",
    "\n",
    "と表されます。単なる成分ごとの掛け算なのですが、\n",
    "- ${\\bf g}$ の各成分が何らかの特徴量になっていて\n",
    "- 成分積を取ることで その各成分を強調したり、減衰させたりする\n",
    "\n",
    "役割を持たせたい、という気持ちを込めて、今回は絵で書く際は\n",
    "\n",
    "![alt](gate.jpg)\n",
    "\n",
    "のように、ベクトルの各成分を電気回路に模して描き、ゲートベクトルはゲート処理の大きさの度合いを表す「スイッチ」のように表現してみます。ここで重要なのは、ゲートベクトルの値はネットワーク内で訓練によって獲得されるという点です。訓練の目的に応じて、何らかの入力クエリから ${\\bf g}$ は決定されるような構造をしています。\n",
    "> ゲートの考え方は、**クエリによって注目すべき場所を変える** という思想が、後で紹介する**注意機構(attention mechanism)** と近い考え方をしていることがわかります。\n",
    "\n",
    "この記法を用いると LSTM(を含んだ全体のネットワーク) は以下のように描けます：\n",
    "\n",
    "\n",
    "![alt](rnn4.jpg)\n",
    "\n",
    "sigmoid関数のマークから点線が各ゲートに向かって描かれていますが、それはsigmoid出力ベクトルをゲートベクトルに使うと言う意味で、sigmoidに入ってくるそれぞれのベクトルがそれぞれのゲートの処理を制御するための入力クエリとなります。素朴なLSTMは図で示してあるように、3種類のゲートと1つの入力処理（唯一 `tanh` が活性化関数に使われている部分）、出力ベクトルとメモリベクトル、から成ります。それぞれのゲートは\n",
    "* ${\\bf g^{forget}}$：**忘却ゲート** と呼ばれ、現在の時刻のクエリ(${\\bf h}_t, {\\bf e}_t$)に応じて、前時刻のメモリ ${\\bf c}_{t-1}$ のうちどの成分を忘れるべきかを決定する\n",
    "* ${\\bf g^{input}}$：**入力ゲート** と呼ばれ、現在の時刻のクエリ(${\\bf h}_t, {\\bf e}_t$)に応じて、`tanh`による入力処理の成分のうちどれをメモリに書き込むべきかを決定する\n",
    "* ${\\bf g^{output}}$：**出力ゲート** と呼ばれ、現在の時刻のクエリ(${\\bf h}_t, {\\bf e}_t$)に応じて、メモリ ${\\bf c}_t$ のうちどの成分を呼び出せばよいかを決定する\n",
    "\n",
    "といった働きをしていると期待されます。\n",
    "> これらのゲート処理は計算機の実装を思い起こさせます。実際、実はRNNを用いると、任意のアルゴリズムの実装が原理的には可能（たった一層でもチューリング完全）なのですが、これを凡例から**学習で獲得させる**となると、LSTMだけだとうまく行かないことが知られています。これはメモリの読み書きを「ベクトルの成分」で実装しているため、メモリの処理で重要な情報が上書きされやすいせいだと考えられます。後の節#で、アルゴリズム学習に特化した、より改善されたネットワーク構造を説明します。\n",
    "\n",
    "具体的にはLSTMによる ${\\color{red}{f_{rnn}}}({\\bf e}_t)$ は以下で定義されます：\n",
    "\n",
    "$$\n",
    "{\\color{red}{f_{rnn}}}({\\bf e}_t) := {\\bf h}_t\n",
    "\\left\\{ \\begin{array}{ll}\n",
    "{\\bf i}_t = \\tanh ({\\color{red}{W_{e}}} {\\bf e}_t + {\\color{red}{W_{h}}}{\\bf h}_{t-1} + {\\color{red}{{\\bf b}}}  )  & {\\text{Input}}\n",
    "\\\\\n",
    "{\\bf c}_t = {\\bf c}_{t-1} \\odot \\underbrace{{\\bf \\sigma}_{sigmoid}({\\color{red}{W_{fe}}{\\bf e}_t + {\\color{red}{W_{fh}} {\\bf h}_{t-1}  }} + {\\color{red}{{\\bf b}_f}})}_{{\\bf g^{forget}}} + {\\bf i}_t \\odot \\underbrace{{\\bf \\sigma}_{sigmoid}({\\color{red}{W_{ie}}{\\bf e}_t + {\\color{red}{W_{ih}} {\\bf h}_{t-1}  }}+ {\\color{red}{{\\bf b}_i}})}_{\\bf g^{input}} & {\\text{Memory Cell update}}\n",
    "\\\\\n",
    "{\\bf h}_t = \\tanh({\\bf c}_t) \\odot \\underbrace{{\\bf \\sigma}_{sigmoid}({\\color{red}{W_{oe}}{\\bf e}_t + {\\color{red}{W_{oh}} {\\bf h}_{t-1}  }}+ {\\color{red}{{\\bf b}_o}})}_{\\bf g^{output}}  & {\\text{Output \\& h-update}}\n",
    "\\end{array} \\right.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  LSTMで訓練する\n",
    "`MyRNN`の構造は同じで、上で`RNN_layer=SimpleRNN`だったところをLSTMに変えるだけです。さっきの`train_RNN()`で`RNN_layer=LSTM`のオプションを付けると勝手に\"RNN_test\"で始まるファイル名で訓練済みモデルのパラメータが保存されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.9 s, sys: 2.93 s, total: 20.9 s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss_averages = train_RNN(RNN_layer = LSTM, epoch_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUで実行すると、tensorflowのデフォルト設定だと[cuDNN](https://developer.nvidia.com/cudnn)をうまく使ってくれる実装にしてあるようで、`SimpleRNN`よりも構造が複雑なくせに訓練が終わるのが早いです。（CPUだとこちらのほうが遅い？）\n",
    "\n",
    "#### LSTMでシェイクスピア台詞生成\n",
    "こちらも、同じ関数でオプションを変えることで上で訓練したLSTMを使った台詞生成が可能です："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATHULEY:\n",
      "Very by the gane of him.\n",
      "*\n",
      "Ad\n",
      "By this, in my soply ragainio.\n",
      "*\n",
      "Ad\n",
      "Ischear yours!\n",
      "*\n",
      "ANLEST:\n",
      "I'll be pay thee, or it: and then.\n",
      "*\n",
      "AMERLIUS:\n",
      "Ay, give me as thing it be dase.\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "model=load_RNN(RNN_layer = LSTM)\n",
    "\n",
    "for _ in range(5):\n",
    "    model.sample_from(\"A\", num_string=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "たまに変な単語が現れたりしますが、かなり自然な台詞っぽく見えるようになりました。ただし、お気付きの通り文法は変なままです。文法についてはもう少し工夫した訓練をしないと無理かもしれません。\n",
    "### ゲート付き再帰ユニット(Gated Recurrent Unit, GRU)\n",
    "GRUは近年提案されたユニットです([arXiv:1406.1078](https://arxiv.org/abs/1406.1078))。LSTMと同様、メモリとゲートの導入したユニットです。まずは全体図をお見せします：\n",
    "\n",
    "![alt](rnn3.jpg)\n",
    "\n",
    "GRUでは、LSTMの軽量化が図られているのがわかります。実際、出力ベクトル ${\\bf h}_t$ は最早ユニット内で再帰せず、${\\bf c}_t$ だけが再帰構造を持っています。加えて、LSTMには3つあったゲートが2つに減らされています。それぞれ\n",
    "* ${\\bf g^{reset}}$：**リセットゲート**、現在の入力 ${\\bf e}_t$ と一時刻前のセル ${\\bf c}_{t-1}$ をクエリとし、一時刻前のセル ${\\bf c}_{t-1}$ をどれくらい現在の入力に入れるべきかを決定する\n",
    "* ${\\bf g^{update}}$：**アップデートゲート**、現在の入力 ${\\bf e}_t$ と一時刻前のセル ${\\bf c}_{t-1}$ をクエリとし、一時刻前のセル ${\\bf c}_{t-1}$ と 現在の出力 ${\\bf h}_t$ の混合の仕方を決定する。混合されたベクトルが現在のセル ${\\bf c}_t$ となる\n",
    "\n",
    "ものすごく大雑把に言うと、LSTMのゲートとGRUのゲートの対応は\n",
    "\n",
    ".|LSTMのゲート|GRUのゲート\n",
    "---|:---:|:---:\n",
    "出力に関する処理|${\\bf g^{forget}, g^{output}}$|${\\bf g^{reset}}$\n",
    "メモリに関する処理|${\\bf g^{forget}, g^{input}}$|${\\bf g^{update}}$\n",
    "\n",
    "のようになっています。ゲートが減った分、LSTMよりも軽いユニットになっています。式をかくと以下のようになっています：\n",
    "\n",
    "\n",
    "$$\n",
    "{\\color{red}{f_{rnn}}}({\\bf e}_t) := {\\bf h}_t\n",
    "\\left\\{ \\begin{array}{ll}\n",
    "{\\bf h}_t = \\tanh ({\\color{red}{W_{he}}} {\\bf e}_t + {\\color{red}{W_{hrc}}} [\\underbrace{{\\bf \\sigma}_{sigmoid}({\\color{red}{W_{re}}{\\bf e}_t + {\\color{red}{W_{rh}} {\\bf c}_{t-1}  }})}_{{\\bf g^{reset}}} \\odot {\\bf c}_{t-1}] )  & {\\text{Output}}\n",
    "\\\\\n",
    "{\\bf c}_t = \\underbrace{{\\bf \\sigma}_{sigmoid}({\\color{red}{W_{ue}}{\\bf e}_t + {\\color{red}{W_{uh}} {\\bf c}_{t-1}  }})}_{{\\bf g^{update}}}\\odot({\\bf c}_{t-1} - {\\bf h}_t) + {\\bf h}_t = {\\bf g^{update}} \\odot {\\bf c}_{t-1} + [{\\bf 1-  g^{update}}] \\odot {\\bf h}_t & \\text{Cell update}\n",
    "\\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  GRUで訓練する\n",
    "同上です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.8 s, sys: 2.68 s, total: 19.4 s\n",
      "Wall time: 25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loss_averages = train_RNN(RNN_layer = GRU, epoch_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMより訓練完了が、やや早いのがわかります。\n",
    "\n",
    "#### GRUでシェイクスピア台詞生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND UpERENDurga.\n",
      "*\n",
      "Able done: there's no mercy.\n",
      "*\n",
      "AND Lord Angelo!\n",
      "*\n",
      "AND Kpepprest: the dobe conselss will thrie bands.\n",
      "*\n",
      "AQTHAMENLUS:\n",
      "Are you there, the more has on the man as he.\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "model=load_RNN(RNN_layer = GRU)\n",
    "\n",
    "for _ in range(5):\n",
    "    model.sample_from(\"A\", num_string=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようにGRUでもそれなりにシェイクスピアっぽく見える文章生成を作れることがわかります。\n",
    "\n",
    "### Licence in this subsection\n",
    "In this notebook, the definitions of\n",
    "\n",
    "* class method: `Generator.sample_from()`\n",
    "* function: `get_data_dicts()`\n",
    "* class: `MyRNN`\n",
    "* function: `load_RNN`\n",
    "\n",
    "include codes derived from\n",
    "https://www.tensorflow.org/tutorials/text/text_generation\n",
    "which is licenced under [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see their [Site Policies](https://developers.google.com/terms/site-policies).\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
